{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a5b2469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n! Question 3: KNN vs Logistic Regression on Medication Persistency\\n\\n* This notebook will:\\n- Load and preprocess the Persistent_vs_NonPersistent dataset\\n- Baseline: Logistic Regression (5-fold CV + test split)\\n- Find best k via elbow method\\n- Train/evaluate KNN (5-fold CV + test split)\\n- Compare accuracy, precision, recall, F1\\n- Reflect on why one model outperforms the other\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "! Question 3: KNN vs Logistic Regression on Medication Persistency\n",
    "\n",
    "* This notebook will:\n",
    "- Load and preprocess the Persistent_vs_NonPersistent dataset\n",
    "- Baseline: Logistic Regression (5-fold CV + test split)\n",
    "- Find best k via elbow method\n",
    "- Train/evaluate KNN (5-fold CV + test split)\n",
    "- Compare accuracy, precision, recall, F1\n",
    "- Reflect on why one model outperforms the other\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c9a9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "! Part 1: Load and preprocess the Persistent_vs_NonPersistent dataset\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports and data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv('./res/Persistent_vs_NonPersistent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24d08944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3424, 69)\n",
      "Persistency_Flag\n",
      "Non-Persistent    2135\n",
      "Persistent        1289\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "! Part 2: EDA & Target encoding\n",
    "\"\"\"\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(df['Persistency_Flag'].value_counts(), \"\\n\")\n",
    "\n",
    "df['target'] = df['Persistency_Flag'].map({'Persistent': 1, 'Non-Persistent': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "decabc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "! Part 3: Preprocessing\n",
    "* - Drop unnecessary columns and target\n",
    "* - one-hot encode categorical features\n",
    "* - Scale numeric\n",
    "\"\"\"\n",
    "\n",
    "# ? Drop unnecessary columns and target\n",
    "X = df.drop(['Ptid', 'Persistency_Flag', 'target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# * One-hot encode categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# * Scale Count_Of_Risks for KNN\n",
    "# ? Note: KNN is sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "if 'Count_Of_Risks' in X:\n",
    "    X['Count_Of_Risks'] = scaler.fit_transform(X[['Count_Of_Risks']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8995b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "! Part 4: Train/Test Split\n",
    "\"\"\"\n",
    "\n",
    "# * Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "643998b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  {'accuracy': np.float64(0.8185450833344452), 'precision': np.float64(0.7907512323801387), 'recall': np.float64(0.7061007526366361), 'f1': np.float64(0.7444818492210639)}\n",
      "Cross-validation results:  {'accuracy': 0.7985401459854015, 'precision': 0.7564102564102564, 'recall': 0.686046511627907, 'f1': 0.7195121951219512}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "! Part 5: Baseline: Logistic Regression\n",
    "\"\"\"\n",
    "\n",
    "# * Logistic Regression\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log_cv = {\n",
    "    'accuracy': cross_val_score(log, X_train, y_train, cv=cv, scoring='accuracy').mean(),\n",
    "    'precision': cross_val_score(log, X_train, y_train, cv=cv, scoring='precision').mean(),\n",
    "    'recall': cross_val_score(log, X_train, y_train, cv=cv, scoring='recall').mean(),\n",
    "    'f1': cross_val_score(log, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "}\n",
    "\n",
    "# * Fit and predict\n",
    "log.fit(X_train, y_train)\n",
    "y_pred_log = log.predict(X_test)\n",
    "log_test = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_log),\n",
    "    'precision': precision_score(y_test, y_pred_log),\n",
    "    'recall': recall_score(y_test, y_pred_log),\n",
    "    'f1': f1_score(y_test, y_pred_log)\n",
    "}\n",
    "\n",
    "# * Print results\n",
    "print(\"Logistic Regression: \", log_cv)\n",
    "print(\"Cross-validation results: \", log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0071e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k (by CV accuracy): 13\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "! Part 6: Elbow method to pick k for KNN\n",
    "\"\"\"\n",
    "\n",
    "# * Accuracy list for different k values\n",
    "accuracy = []\n",
    "k_list = range(1, 21)\n",
    "\n",
    "# * Loop through k values and calculate accuracy\n",
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=cv, scoring='accuracy').mean()\n",
    "    accuracy.append(scores)\n",
    "    \n",
    "# * Plot accuracy for k\n",
    "best_k = k_list[np.argmax(accuracy)]\n",
    "print(\"Best k (by CV accuracy):\", best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f537bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:  {'accuracy': np.float64(0.7944521544189274), 'precision': np.float64(0.8347310300443377), 'recall': np.float64(0.5688976107016204), 'f1': np.float64(0.6754004874991552)}\n",
      "Cross-validation results:  {'accuracy': 0.7970802919708029, 'precision': 0.84, 'recall': 0.5697674418604651, 'f1': 0.6789838337182448}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "! Part 7: Train and evaluate KNN with best k\n",
    "\"\"\"\n",
    "\n",
    "# * Train KNN with best k\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_cv = {\n",
    "    'accuracy': cross_val_score(knn, X_train, y_train, cv=cv, scoring='accuracy').mean(),\n",
    "    'precision': cross_val_score(knn, X_train, y_train, cv=cv, scoring='precision').mean(),\n",
    "    'recall': cross_val_score(knn, X_train, y_train, cv=cv, scoring='recall').mean(),\n",
    "    'f1': cross_val_score(knn, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "}\n",
    "\n",
    "# * Fit and predict\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "knn_test = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_knn),\n",
    "    'precision': precision_score(y_test, y_pred_knn),\n",
    "    'recall': recall_score(y_test, y_pred_knn),\n",
    "    'f1': f1_score(y_test, y_pred_knn)\n",
    "}\n",
    "\n",
    "# * Print results\n",
    "print(\"KNN: \", knn_cv)\n",
    "print(\"Cross-validation results: \", knn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2284bb",
   "metadata": {},
   "source": [
    "### Comparison and Insights\n",
    "\n",
    "**Logistic Regression Results:**\n",
    "- **Cross-Validation (CV):**\n",
    "  - Accuracy: ~0.794\n",
    "  - Precision: ~0.835\n",
    "  - Recall: ~0.569\n",
    "  - F1 Score: ~0.675\n",
    "- **Test Set:**\n",
    "  - Accuracy: ~0.797\n",
    "  - Precision: ~0.84\n",
    "  - Recall: ~0.57\n",
    "  - F1 Score: ~0.68\n",
    "\n",
    "**K-Nearest Neighbors (KNN) Results (k=13):**\n",
    "- **Cross-Validation (CV):**\n",
    "  - Accuracy: ~0.79\n",
    "  - Precision: ~0.83\n",
    "  - Recall: ~0.57\n",
    "  - F1 Score: ~0.68\n",
    "- **Test Set:**\n",
    "  - Accuracy: ~0.80\n",
    "  - Precision: ~0.84\n",
    "  - Recall: ~0.57\n",
    "  - F1 Score: ~0.68\n",
    "\n",
    "**Key Observations:**\n",
    "1. Logistic Regression outperforms KNN in terms of recall and F1 score, indicating better performance in identifying true positives.\n",
    "2. KNN achieves slightly higher precision, suggesting fewer false positives compared to Logistic Regression.\n",
    "3. Logistic Regression demonstrates more balanced performance across all metrics, while KNN struggles with recall, likely due to its sensitivity to class imbalance and feature scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### Reflection and Evaluation\n",
    "\n",
    "1. **Model Selection:**\n",
    "   - Logistic Regression is a better choice for this dataset due to its higher recall and F1 score, which are critical for applications where identifying true positives is important (e.g., healthcare scenarios like medication persistency).\n",
    "\n",
    "2. **KNN Challenges:**\n",
    "   - KNN's performance is heavily influenced by feature scaling and the choice of `k`. While the elbow method helped identify the optimal `k`, the model still underperformed in recall.\n",
    "   - The high dimensionality of the dataset (116 features) may have also impacted KNN's performance, as it is less effective in high-dimensional spaces.\n",
    "\n",
    "3. **Future Improvements:**\n",
    "   - For Logistic Regression, exploring regularization techniques (e.g., L1 or L2) could further improve performance and interpretability.\n",
    "   - For KNN, dimensionality reduction techniques (e.g., PCA) or feature selection could help mitigate the curse of dimensionality.\n",
    "   - Addressing class imbalance through techniques like oversampling (e.g., SMOTE) or adjusting class weights could improve recall for both models.\n",
    "\n",
    "4. **Conclusion:**\n",
    "   - Logistic Regression is the preferred model for this task, given its consistent and balanced performance across metrics. However, further experimentation and fine-tuning could enhance both models' effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
